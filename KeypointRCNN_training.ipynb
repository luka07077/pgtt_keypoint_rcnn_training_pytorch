{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import albumentations as A # Library for augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more here https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo\n",
    "\n",
    "        self.img_folder = os.path.join(root, \"images\")\n",
    "        self.annot_folder = os.path.join(root, \"annotations\")\n",
    "\n",
    "        # Obtain the file names of all images and JSON files (excluding suffixes)\n",
    "        # Filter out files starting with \". \"\n",
    "        all_img_names = {os.path.splitext(f)[0] for f in os.listdir(self.img_folder) if not f.startswith('.')}\n",
    "        all_ann_names = {os.path.splitext(f)[0] for f in os.listdir(self.annot_folder) if not f.startswith('.')}\n",
    "\n",
    "        # 2. Take the intersection: Only retain the file names that have both images and annotations\n",
    "        self.valid_names = sorted(list(all_img_names & all_ann_names))\n",
    "\n",
    "        # Print out the actual quantity loaded for easy debugging\n",
    "        print(f\"ðŸ“‚ loading {root} ...\")\n",
    "        print(f\"   - Original image: {len(all_img_names)}, Original annotation: {len(all_ann_names)}\")\n",
    "        print(f\"   - âœ… Effective pairing: {len(self.valid_names)}\")\n",
    "\n",
    "        # 3. Establish a mapping to facilitate the subsequent retrieval of complete file names with suffixes based on the names\n",
    "        self.filename_map = {}\n",
    "        for f in os.listdir(self.img_folder):\n",
    "            name, ext = os.path.splitext(f)\n",
    "            if name in self.valid_names:\n",
    "                self.filename_map[name] = f\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Obtain the \"base file name\" corresponding to the current index (without a suffix)\n",
    "        base_name = self.valid_names[idx]\n",
    "\n",
    "        # 2. Construct an absolute path\n",
    "        img_filename = self.filename_map[base_name]\n",
    "        img_path = os.path.join(self.img_folder, img_filename)\n",
    "        annotations_path = os.path.join(self.annot_folder, base_name + \".json\")\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "\n",
    "            # Category name\n",
    "            bboxes_labels_original = ['Tower' for _ in bboxes_original]\n",
    "\n",
    "        if self.transform:\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "\n",
    "            # Automatically adapt the number of key points\n",
    "            num_kps = len(keypoints_original[0])\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1, num_kps, 2)).tolist()\n",
    "\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj):\n",
    "                    # Protective measures: Prevent index out-of-bounds\n",
    "                    if k_idx < len(keypoints_original[o_idx]):\n",
    "                        visibility = keypoints_original[o_idx][k_idx][2]\n",
    "                        obj_keypoints.append(kp + [visibility])\n",
    "                    else:\n",
    "                        obj_keypoints.append(kp + [1])\n",
    "                keypoints.append(obj_keypoints)\n",
    "\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64)\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)\n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length here is the length of the intersection and will never cross the boundary\n",
    "        return len(self.valid_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing a random item from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = 'pgtt_keypoints_dataset_imgs/drum/train'\n",
    "\n",
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "print(\"Transformed targets:\\n\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 18\n",
    "    color_blue = (0, 0, 255)\n",
    "    color_outline = (255, 255, 255)\n",
    "\n",
    "    font_type = cv2.FONT_HERSHEY_DUPLEX\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "\n",
    "    # ================= Helper Function: Draw Paired Keypoints =================\n",
    "    def draw_pairs(img, kps):\n",
    "        # We assume there are 22 points in total, divided into 11 pairs\n",
    "        # Iterate through each level (level 1 to 11)\n",
    "        # range(0, 22, 2) means taking indices 0, 2, 4, ... 20\n",
    "        for i in range(0, len(kps), 2):\n",
    "            if i + 1 >= len(kps): break # Prevent index out of bounds\n",
    "\n",
    "            # Extract the pair of points\n",
    "            pt1 = tuple(kps[i])     # Could be left or right\n",
    "            pt2 = tuple(kps[i+1])   # Could be left or right\n",
    "\n",
    "            # Current level (1, 2, ... 11)\n",
    "            level = (i // 2) + 1\n",
    "\n",
    "            # Core logic: Compare X coordinates\n",
    "            # pt[0] is x, pt[1] is y\n",
    "            if pt1[0] < pt2[0]:\n",
    "                # pt1 is on the left, pt2 is on the right\n",
    "                left_pt, right_pt = pt1, pt2\n",
    "            else:\n",
    "                # pt1 is on the right, pt2 is on the left (swap)\n",
    "                left_pt, right_pt = pt2, pt1\n",
    "\n",
    "            # Define labels\n",
    "            label_left = f\"l{level}\"\n",
    "            label_right = f\"r{level}\"\n",
    "\n",
    "            # === Draw Left Point ===\n",
    "            img = cv2.circle(img, left_pt, 3, color_blue, -1)\n",
    "            cv2.putText(img, label_left, left_pt, font_type, font_scale, color_outline, font_thickness + 2, cv2.LINE_AA)\n",
    "            cv2.putText(img, label_left, left_pt, font_type, font_scale, color_blue, font_thickness, cv2.LINE_AA)\n",
    "\n",
    "            # === Draw Right Point ===\n",
    "            img = cv2.circle(img, right_pt, 3, color_blue, -1)\n",
    "            cv2.putText(img, label_right, right_pt, font_type, font_scale, color_outline, font_thickness + 2, cv2.LINE_AA)\n",
    "            cv2.putText(img, label_right, right_pt, font_type, font_scale, color_blue, font_thickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "    # ================= Main Drawing Logic =================\n",
    "\n",
    "    # 1. Draw Bounding Boxes\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "\n",
    "    # 2. Draw Keypoints (Using new paired drawing logic)\n",
    "    for kps in keypoints:\n",
    "        image = draw_pairs(image, kps)\n",
    "\n",
    "    # 3. Display Logic\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Comparison display for training set\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "\n",
    "        for kps in keypoints_original:\n",
    "            image_original = draw_pairs(image_original, kps)\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "\n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "\n",
    "    # 1. Define custom Anchor Generator to handle various object scales and ratios\n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "\n",
    "    # 2. Build the model with the custom configuration\n",
    "    # num_classes=2 (Background + Drum tower)\n",
    "    # num_keypoints=22 (Specific to Drum tower structure)\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2,\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = 'pgtt_keypoints_dataset_imgs/drum/train'\n",
    "KEYPOINTS_FOLDER_TEST = 'pgtt_keypoints_dataset_imgs/drum/test'\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model(num_keypoints = 22)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.3)\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_test, device)\n",
    "    \n",
    "# Save model weights after training\n",
    "torch.save(model.state_dict(), 'keypointsrcnn_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualizing model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iterator)\n",
    "images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(images)\n",
    "\n",
    "print(\"Predictions: \\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image, bboxes, keypoints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
